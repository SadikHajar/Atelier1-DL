{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from pandas import read_csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# dataset definition preparation \n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        self.df = read_csv(path)\n",
    "\n",
    "        self.df.head()\n",
    "        # Drop the date column\n",
    "        self.df = self.df.drop('date', axis=1)\n",
    "        # Drop the symbol column\n",
    "        self.df = self.df.drop('symbol', axis=1)\n",
    "        # store the inputs and outputs\n",
    "        self.X = self.df.values[:, :-1]\n",
    "        self.y = self.df.values[:, -1]\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.df.info())\n",
    "        print(self.df.describe())\n",
    "        print(self.df.head())\n",
    "        print(self.df.isnull().sum())\n",
    "\n",
    "    def visualize(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y='open', data=self.df)\n",
    "        plt.title('Spans from 2010 to the end 2016')\n",
    "        plt.show()\n",
    "\n",
    "    def check(self):\n",
    "        # Step 3: Check for missing values\n",
    "        print(self.df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "21143567cd556d5b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data_set = CSVDataset('data/prices.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2f2810fefabbe46d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: Summarize the dataset\n",
    "data_set.print_info()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c640e4c76b97e9af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: Visualize the dataset\n",
    "data_set.visualize()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "663427805d37b492",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Check for missing values\n",
    "data_set.check()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e6d85c84433b8ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "\n",
    "\n",
    "# Step 5: Define the DNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_neurons=10):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the layers of the network here\n",
    "        self.fc1 = nn.Linear(5, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.relu(self.fc2(x))\n",
    "        # no activation function for the la\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "13f2aa4136e7e042",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 6: Create an instance of the network\n",
    "model = Net()\n",
    "\n",
    "\n",
    "# Step 7: Train the model\n",
    "def train_model(train_data, training_model):\n",
    "    size = len(train_data.dataset)\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    # enumerate epochs\n",
    "    for epoch in tqdm(range(100), desc='Training Epochs'):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        # enumerate mini batches\n",
    "        for batch, (inputs, targets) in enumerate(train_data):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = training_model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            #if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(inputs)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b8be6151d6f344fa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 8: Prepare the data\n",
    "def prepare_data(dataset):\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    return DataLoader(train, batch_size=1024, shuffle=True), DataLoader(test, batch_size=1024, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c1ba0d06e53310dc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 9: Train the model\n",
    "train_dl, test_dl = prepare_data(data_set)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "train_model(train_dl, model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1206287323dd2c1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Step 10: Define a function to create a PyTorch model with variable parameters\n",
    "def create_model(n_neurons=10):\n",
    "    return Net(n_neurons=n_neurons)\n",
    "\n",
    "\n",
    "# Step 11: Define a function to perform grid search\n",
    "def perform_grid_search(X, y):\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'module__n_neurons': [10, 20, 30],\n",
    "        'lr': [0.01, 0.001, 0.0001],\n",
    "        'max_epochs': [10, 50, 100],\n",
    "        'optimizer': [optim.SGD, optim.Adam],\n",
    "    }\n",
    "\n",
    "    # Create a Skorch neural network object\n",
    "    net = NeuralNetRegressor(module=create_model, criterion=nn.MSELoss, iterator_train__shuffle=True)\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(net, hyperparameters, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Step 12: Call the grid search function with the desired parameters\n",
    "best_model = perform_grid_search(data_set.X, data_set.y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bd08999c05aacba3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 13: Initialize lists to store loss and accuracy values\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "# Step 14: Train the model\n",
    "def test_model(test_data, testing_model):\n",
    "    size = len(test_data.dataset)\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    current_test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_data:\n",
    "            # compute the model output\n",
    "            outputs = testing_model(inputs)\n",
    "            # calculate loss\n",
    "            current_test_loss += criterion(outputs, targets).item()\n",
    "            # calculate accuracy\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    current_test_loss /= size\n",
    "    current_test_accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * current_test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train the model\n",
    "    train_loss, train_accuracy = train_model(train_dl, model)\n",
    "    # Test the model\n",
    "    test_loss, test_accuracy = test_model(test_dl, model)\n",
    "    # Store the loss and accuracy values\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Step 15: After training, use matplotlib to plot the loss and accuracy values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.title('Loss / Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.title('Accuracy / Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eeca4a7ce36634e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetWithRegularization(nn.Module):\n",
    "    def __init__(self, n_neurons=10, dropout_rate=0.5, *args, **kwargs):\n",
    "        super(NetWithRegularization, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, n_neurons)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(train_data, validation_data, training_model, l1_lambda=0.005):\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)  # L2 regularization\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in tqdm(range(100), desc='Training Epochs'):\n",
    "        # Training Phase \n",
    "        training_model.train()\n",
    "        for inputs, targets in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = training_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1 regularization\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in validation_data:\n",
    "                outputs = training_model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5a1946af31e2fefb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the original model\n",
    "test_loss_original, test_accuracy_original = test_model(test_dl, model)\n",
    "print(f\"Original Model - Test Loss: {test_loss_original}, Test Accuracy: {test_accuracy_original}\")\n",
    "\n",
    "# Create an instance of the network with regularization\n",
    "model_with_regularization = NetWithRegularization()\n",
    "\n",
    "# Train the model with regularization\n",
    "train_model(train_dl, test_dl, model_with_regularization)\n",
    "\n",
    "# Test the model with regularization\n",
    "test_loss_regularized, test_accuracy_regularized = test_model(test_dl, model_with_regularization)\n",
    "print(f\"Model with Regularization - Test Loss: {test_loss_regularized}, Test Accuracy: {test_accuracy_regularized}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1758c75bc54c50ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can summarize the first part of the lab in those steps:  \n",
    "\n",
    "1. A CSV dataset is loaded and preprocessed. The date and symbol columns are dropped, and the remaining data is split into inputs (X) and targets (y).\n",
    "\n",
    "2. A neural network architecture is defined with three fully connected layers.\n",
    "\n",
    "3. The model is trained using Stochastic Gradient Descent (SGD) and Mean Squared Error (MSE) loss.\n",
    "\n",
    "4. A grid search is performed to find the best hyperparameters for the model.\n",
    "\n",
    "5. The model's performance is evaluated on the test data.\n",
    "\n",
    "6. A new model is defined with dropout layers for regularization. The training function is also modified to include L1 and L2 regularization and early stopping.\n",
    "\n",
    "7. The regularized model is trained and evaluated on the test data, and its performance is compared with the original model.\n",
    "\n",
    "8. The loss and accuracy of the models are plotted for each epoch.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8819bbfbddd29185"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
